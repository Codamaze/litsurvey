[
    {
        "title": "Large Language Models for Indian Legal Text Summarisation",
        "authors": [
            "Hemanth Kumar M",
            "Jayanth P",
            "Anand Kumar M"
        ],
        "year": 2024,
        "abstract": "Summarizing legal case judgments is a complex task in Legal Natural Language Processing (NLP), with a gap in understanding how various summarization models, including extractive and abstractive approaches and analysing the perform within the domain of legal documents. Since there are around 4 crore pending cases in the Indian court system, this study addresses the challenge of laborious task of manually summarizing legal documents. It introduces both supervised and unsupervised models for both extractive and abstractive summarization, showcasing their effective performance through evaluations using ROUGE metrics and BERT score. BART, T5, PEGASUS, ROBERTA, Legal-PEGASUS, Legal-BERT models are used for abstractive summarisation. TextRank, LexRank, LSA, Summarizer BERT, KL-Summ are used in case of extractive summarisation. Longformer, Bert - Legal Pegasus are also considered for the task of Summarisation. In the domain of legal document summarization, we used GPT-4 and LLAMA-2, employing prompt engineering with both Zero-shot and Oneshot prompts to extract summaries. As far of our knowledge, this is the first paper that used Large Language Models like GPT-4 and LLama-2 for the task of Legal Text summarisation. Along with that a user-friendly chatbot has been developed utilizing the Llama model and specifically designed to respond for queries related to legal texts. Additionally, a web application has been created, allowing users to upload legal documents for summarization. An option is given to users to select from various languages including Telugu, Tamil, Kannada, Malayalam, and Hindi. As a result the summarised text is converted into respective language.",
        "url": "https://www.semanticscholar.org/paper/6d32af48e9148568265ae885d88df503ace933cf"
    },
    {
        "title": "Abstractive Text Summarisation Using Keywords with Transformers Model",
        "authors": [
            "P. Shanmugam S",
            "Mithul Sudharsan R S",
            "Vignesh P S",
            "Ashwin S S"
        ],
        "year": 2023,
        "abstract": "The application of linguistics to machine learning models has changed text summarization techniques This abstractive method is used to give the best possible summary of huge data without losing the important key points in it. We will be using the BERT encoder for encoding the input text and collecting the vector of the important sentence in the paragraph and that vector will be sent to Transformers for decoding into a text. This text will be created abstractedly using the transform decoder model. For a long time, automatic text summarization has been the focus of natural language processing (NLP) research. The goal of abstractive text summarising is to produce a summary of a given document that only includes the most crucial and pertinent information. Among deep learning models, BERT and transformer models have dominated abstractive text summarization in recent years. This work offers a thorough analysis of transformer-based and BERT abstractive text summarization. We also go through how these models are developed and trained, as well as the many methods for bringing context and subject-matter expertise into the summarising procedure.",
        "url": "https://www.semanticscholar.org/paper/f14a7bb8d2d7d7db09cc034bab8b6e343f5e76f2"
    }
]